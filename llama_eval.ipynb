{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding)\n",
    "from sklearn.utils import shuffle\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_file = \"data/IBC/ibc.csv\"\n",
    "sample_file = \"data/IBC/sample_ibc.csv\"\n",
    "ibc = pd.read_csv(full_file)\n",
    "sample = pd.read_csv(sample_file)\n",
    "# ibc = shuffle(pd.read_csv(full_file), random_state=1)\n",
    "\n",
    "dsq = sample[\"sentence\"].to_list()\n",
    "# print(dsq)\n",
    "ft_ibc = ibc.loc[~ibc[\"sentence\"].isin(dsq), :].copy()\n",
    "# print(ft_ibc.label.value_counts(), ibc.label.value_counts())\n",
    "\n",
    "ft_ibc = shuffle(ft_ibc, random_state=1)\n",
    "ft_ibc.iloc[0]\n",
    "\n",
    "options = [\"liberal\", \"neutral\", \"conservative\"]\n",
    "\n",
    "def add_to_dataset(dataset, sentence, label):\n",
    "    if label == 'liberal':\n",
    "        result = 0\n",
    "    elif label == 'neutral':\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 2\n",
    "\n",
    "    data = {\"sentence\": sentence,\n",
    "            \"label\": result}\n",
    "    dataset.append(data)\n",
    "\n",
    "sample_dataset = []\n",
    "\n",
    "for index in range(len(sample)):\n",
    "    sentence = ft_ibc.iloc[index][\"sentence\"]\n",
    "    add_to_dataset(sample_dataset, sentence, ft_ibc.iloc[index][\"label\"].lower())\n",
    "\n",
    "sample_ex = sample_dataset[0]\n",
    "sample_ex\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for index in range(len(ft_ibc)):\n",
    "    sentence = ft_ibc.iloc[index][\"sentence\"]\n",
    "    add_to_dataset(dataset, sentence, ft_ibc.iloc[index][\"label\"].lower())\n",
    "\n",
    "example = dataset[0]\n",
    "example\n",
    "\n",
    "test_split = (int) (0.1*len(dataset))\n",
    "test_set = dataset[:test_split]\n",
    "train_set = dataset[test_split:]\n",
    "\n",
    "print(f\"Size of test set: {len(test_set)}, size of train set: {len(train_set)}, no overlap: {len(train_set)+len(test_set)==len(dataset)}\")\n",
    "\n",
    "# load into Datasets\n",
    "train_ds = Dataset.from_pandas(pd.DataFrame(data=train_set))\n",
    "test_ds = Dataset.from_pandas(pd.DataFrame(data=test_set))\n",
    "\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = train_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "# Fine-tuning arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"First, government plays an important role.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs375",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
